{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965b2dc7-1e7a-4489-817c-25ecaefeb5c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Max.AI LLM Components for Agent Development\n",
    "**Author:** `Max.AI`\n",
    "\n",
    "This document outlines the modules of the Max.AI LLM accelerators. \n",
    "An individual Data Scientist can leverage Max in-built accelerators to build Gen AI LLM agents. Data scientists have access to Max Development Enviornments, which are equipped with pre-installed Max accelerators. These accelerators are crafted to streamline LLM-based operations such as parsing, extraction, chunking, vectorization, embeddings, retrieval, and generation.\n",
    "\n",
    "\n",
    "#### **Components**:\n",
    "1. `MaxaiLLM`\n",
    "    - This module loads various large language models (LLMs), offering a unified interface for both open-source and online LLMs.The llm component is responsible for loading different llms\n",
    "    \n",
    "  \n",
    "2. `MaxExtractor`\n",
    "   - This package extracts relevant text from unstructured formats like PDFs, PPTX, TXT, and DOCX files, turning complex documents into usable data.\n",
    "\n",
    "\n",
    "3. `MaxSplitter`\n",
    "   - This component breaks down text into smaller units such as paragraphs for detailed analysis and processing. It also extracts metadata from the text.\n",
    "\n",
    "\n",
    "4. `MaxEmbeddings`\n",
    "   - Responsible for converting text or tokens into dense vector representations, this module is vital for NLP tasks like classification or similarity analysis.\n",
    "\n",
    "5. `MaxVectorization`\n",
    "   - MaxVectorization transforms data into a vector format, which is crucial for various machine learning and natural language processing tasks.\n",
    "\n",
    "\n",
    "6. `MaxRetriever`\n",
    "   - MaxRetriever is associated with information retrieval tasks. This module searches for relevant documents or information within a large corpus based on specific queries or contexts.\n",
    "\n",
    "\n",
    "7. `MaxGenerator`\n",
    "   - This component is designed for text generation tasks, including summarization, text completion, and creative text generation based on given prompts or contexts.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46150557-432b-4a1c-9248-c09984869c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-27T19:32:42.477572Z",
     "iopub.status.busy": "2023-11-27T19:32:42.477019Z",
     "iopub.status.idle": "2023-11-27T19:32:42.480697Z",
     "shell.execute_reply": "2023-11-27T19:32:42.480176Z",
     "shell.execute_reply.started": "2023-11-27T19:32:42.477519Z"
    },
    "tags": []
   },
   "source": [
    "### Step 1 : Initialize the Environment Variables from Config Store\n",
    "\n",
    "Leverage Remote Config Store to load required runtime configurations like - \n",
    "- **LLM API Key**: This includes the desire Cloud LLM Service API Key\n",
    "- **Vector Database Details**: These details captures the database's user, password, name, port, and other relevant information which are required for setting up and connecting to the vector database.\n",
    "- **Experiement Tracking Enviornment (E.g. AWS SageMaker, MLFlow, Google VertexAI etc.)  Credentials** : These configurations enables the Max Monitoring capability, using this each individual steps like parameters, artifacts like models, configurations, metrices can be logged and montitored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "783ada5b-ac5f-4c40-8b47-a7509dba26f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:54.090606Z",
     "iopub.status.busy": "2024-01-30T16:20:54.090238Z",
     "iopub.status.idle": "2024-01-30T16:20:54.699264Z",
     "shell.execute_reply": "2024-01-30T16:20:54.698543Z",
     "shell.execute_reply.started": "2024-01-30T16:20:54.090583Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from maxairesources.config_store.config_store import ConfigStore\n",
    "config_store = ConfigStore(secrets_manager='max/agents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a259b5-231b-42df-a2ec-8cc78ba9ee47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-23T00:06:45.188402Z",
     "iopub.status.busy": "2023-11-23T00:06:45.187513Z",
     "iopub.status.idle": "2023-11-23T00:06:45.192149Z",
     "shell.execute_reply": "2023-11-23T00:06:45.191364Z",
     "shell.execute_reply.started": "2023-11-23T00:06:45.188362Z"
    },
    "tags": []
   },
   "source": [
    "### Step 2 : Setup LLM Flow\n",
    "\n",
    "Loading a Large Language Model (LLM) typically refers to the process of initializing and preparing a pre-trained language model for use in various applications, such as text generation, question answering, or other natural language processing tasks. \n",
    "\n",
    "- **Initialization**: This is the first step where the language model is instantiated. It involves setting up the model with its pre-trained parameters, which define how the model will interpret and generate language.\n",
    "\n",
    "- **Configuration**: The model may need to be configured with specific settings. This can include adjusting parameters for performance, accuracy, or output style. For example, setting the length of generated text, the level of creativity or randomness in responses, or specifying certain language rules.\n",
    "\n",
    "- **Testing and Calibration**: Once loaded, the model may undergo some initial tests to ensure it's functioning as expected. This can involve generating test outputs or running benchmark tasks.\n",
    "\n",
    "- **Ready for Use**: After these steps, the LLM is ready to be used for various applications, such as text completion, question answering, content generation, insight extraction or any custom natural language processing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb13ccb0-6a3c-441e-b3a9-2b87da2d7790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:54.701042Z",
     "iopub.status.busy": "2024-01-30T16:20:54.700720Z",
     "iopub.status.idle": "2024-01-30T16:20:54.721712Z",
     "shell.execute_reply": "2024-01-30T16:20:54.721058Z",
     "shell.execute_reply.started": "2024-01-30T16:20:54.701022Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OpenAI model: gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "from maxaillm.model.llm import MaxOpenAILLM\n",
    "\n",
    "MAX_LLM = \"gpt-3.5-turbo\"\n",
    "\n",
    "\n",
    "\n",
    "llm = MaxOpenAILLM(MAX_LLM).load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "093af727-b67d-4771-8789-308356491227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:54.722938Z",
     "iopub.status.busy": "2024-01-30T16:20:54.722632Z",
     "iopub.status.idle": "2024-01-30T16:20:54.727246Z",
     "shell.execute_reply": "2024-01-30T16:20:54.726678Z",
     "shell.execute_reply.started": "2024-01-30T16:20:54.722917Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fda7434da60>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fda74346df0>, temperature=0.1, openai_api_key='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', openai_proxy='', streaming=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72fd09-89ba-48a1-b07e-5f7f7b1830cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3 : Parsing the files using MaxExtractor\n",
    "\n",
    "Max Extractor provides out-of-box methods for parsing documents of different formats. It provides capabilities like - \n",
    "- **Parsing Documents**: Max Extractor is capable of interpreting and processing a wide range of document formats. This includes common text-based files like PDFs, Word documents (DOCX), text files (TXT), presentation files (PPTX), Markdown (MD) etc. Parsing involves reading these files and converting their contents into a structured plain text format.\n",
    "\n",
    "- **Extracting Metadata**: Beyond just reading the main content of documents, Max Extractor can extract metadata. Metadata is the data about the data, which could include information like the document's author, creation date, modification dates, file size, and other properties that aren't part of the main text content but are important for cataloging, searching, and managing documents.\n",
    "\n",
    "- **Cleaning Documents**: This functionality refers to the tool's ability to process and refine the extracted text. Cleaning might involve removing unnecessary or irrelevant sections, correcting formatting issues, standardizing text for consistency, and eliminating errors or noise. This process is crucial for ensuring that the data used for analysis or other applications is accurate, relevant, and in a usable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc530800-2070-484a-943f-54924a354a9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:54.729184Z",
     "iopub.status.busy": "2024-01-30T16:20:54.728897Z",
     "iopub.status.idle": "2024-01-30T16:20:54.732475Z",
     "shell.execute_reply": "2024-01-30T16:20:54.731914Z",
     "shell.execute_reply.started": "2024-01-30T16:20:54.729165Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from maxaillm.data.extractor.MaxExtractor import MaxExtractor\n",
    "\n",
    "me = MaxExtractor()\n",
    "path = \"path/to/sample.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15e741-3e49-4802-87a0-32c6232891ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 3.1 -  Extracting text for all docs with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3059bb59-a384-41d8-a0a7-9b366fcd3090",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:54.733525Z",
     "iopub.status.busy": "2024-01-30T16:20:54.733277Z",
     "iopub.status.idle": "2024-01-30T16:20:55.646624Z",
     "shell.execute_reply": "2024-01-30T16:20:55.645938Z",
     "shell.execute_reply.started": "2024-01-30T16:20:54.733507Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text, metadata = me.extract_text_metadata(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df616ef2-4eab-4048-9206-734f32dfeac8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 3.2 -  Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "76a57637-9fc3-4597-b9ac-85c68342b0d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:55.647797Z",
     "iopub.status.busy": "2024-01-30T16:20:55.647479Z",
     "iopub.status.idle": "2024-01-30T16:20:55.657447Z",
     "shell.execute_reply": "2024-01-30T16:20:55.656922Z",
     "shell.execute_reply.started": "2024-01-30T16:20:55.647773Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_text = me.clean_text(text, \n",
    "                           dehyphenate=True, \n",
    "                           ascii_only=True, \n",
    "                           remove_isolated_symbols=True, \n",
    "                           compress_whitespace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369ec5f-91ef-4ee0-9300-99f3dbc45850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:27:01.326515Z",
     "iopub.status.busy": "2023-11-16T21:27:01.325638Z",
     "iopub.status.idle": "2023-11-16T21:27:01.330054Z",
     "shell.execute_reply": "2023-11-16T21:27:01.329243Z",
     "shell.execute_reply.started": "2023-11-16T21:27:01.326476Z"
    },
    "tags": []
   },
   "source": [
    "### Step 4 : Chunking Layer\n",
    "\n",
    "This stage involves taking cleaned text as input and performing several key functions:\n",
    "\n",
    " - **Creating Documents**: This module inputs clean, pre-processed text. It then organizes this text into structured documents. This step is crucial for preparing the text for detailed analysis and ensuring that it is in a manageable and analyzable format. It can also split the documents based on its structure like .HTML, and .MD can be extracted based on the structure, or based on semantics.\n",
    " - **Extracting Metadata**: In this phase, the module performs multiple tasks to extract valuable information from the text:\n",
    "\n",
    "> - **Extracting Associated Links**: Identifying and extracting any hyperlinks or references to external sources contained within the text.\n",
    "> - **Summarizing Text**: Generating concise summaries of the chunk to capture the main points or themes, useful for quick overviews or indexing.\n",
    "> - **Entity Extraction**: Identifying and extracting named entities (like names of people, places, organizations) from the chunk. This is a key aspect of understanding the content and context of the documents.\n",
    "> - **Keyword Extraction**: Isolating significant keywords or phrases that are central to the text’s topics or themes. This aids in categorizing and searching the document.\n",
    "> - **Default File-Specific Metadata**: Extracting standard metadata associated with the file, such as author, creation date, file size, etc.\n",
    "- **Return - List of Documents with Content and Metadata for Each Chunk**: The final output of this step is a collection of documents. Each document in this collection contains not only the chunked content but also the extracted metadata. This structured output is primed for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "855ae34a-17f3-480f-a22d-4271cf48d7ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:55.658404Z",
     "iopub.status.busy": "2024-01-30T16:20:55.658168Z",
     "iopub.status.idle": "2024-01-30T16:20:55.661926Z",
     "shell.execute_reply": "2024-01-30T16:20:55.661354Z",
     "shell.execute_reply.started": "2024-01-30T16:20:55.658386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from maxaillm.data.chunking.TextSplitter import TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6812819-624a-407e-86d6-a33a0e81db03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:55.662821Z",
     "iopub.status.busy": "2024-01-30T16:20:55.662636Z",
     "iopub.status.idle": "2024-01-30T16:20:55.666220Z",
     "shell.execute_reply": "2024-01-30T16:20:55.665725Z",
     "shell.execute_reply.started": "2024-01-30T16:20:55.662805Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "splitter = TextSplitter(chunk_size= 4000,chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de120f83-38d5-41ec-9ce4-ae0099519a47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:55.667194Z",
     "iopub.status.busy": "2024-01-30T16:20:55.666940Z",
     "iopub.status.idle": "2024-01-30T16:20:55.675055Z",
     "shell.execute_reply": "2024-01-30T16:20:55.674553Z",
     "shell.execute_reply.started": "2024-01-30T16:20:55.667176Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = splitter.create_documents([text],\n",
    "                                 file_metadata = metadata, \n",
    "                                 metadata = {\n",
    "                                             'default' : True,\n",
    "                                             'summary': False,\n",
    "                                             'entities' : False,\n",
    "                                             \"frequent_keywords\" : False, \n",
    "                                             \"links\" : True\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfab3fb-fd0e-4ff8-9189-eb7617485964",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 5 : Embedding and Vector DB Backend Integration\n",
    "This phase is pivotal for transforming text data into a format that can be efficiently processed and analyzed by large language model. It involves two main tasks:\n",
    "- **Initialize the Embeddings** : \n",
    "> - The first task is to initialize embeddings, which are high-dimensional vector representations of text. These embeddings convert words, phrases, or entire documents into numerical form, capturing the semantic meaning in a way that machines can understand.\n",
    "> - This process often involves using pre-trained models that have learned these representations from large datasets. The initialization ensures that the system is equipped to convert text into these vector formats. We here leverage MaxHuggingFaceEmbeddings, to load any open-source embedding model and generate vectors from text.\n",
    "- **Intitialize the vector DB** : \n",
    "> - The second task is to initialize a vector database. This database is designed to store and manage the vector representations of the text.\n",
    "> - Vector databases like Chroma, MaxPGVector, Milvus, Redis, OpenSearch etc. Each of these systems specializes in handling high-dimensional data.\n",
    "> - Once the vector database is initialized, the documents, now converted into their vectorized forms (embeddings), are pushed into the vector store. This process effectively stores the rich, vectorized representations of the text in a database. The numerical representation of the texts are then leveraged by MaxRetriever module to retrieve response for a particular user query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d973b-2170-4b1f-8580-8d2179080cb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T22:54:59.232609Z",
     "iopub.status.busy": "2023-11-16T22:54:59.231834Z",
     "iopub.status.idle": "2023-11-16T22:54:59.236472Z",
     "shell.execute_reply": "2023-11-16T22:54:59.235605Z",
     "shell.execute_reply.started": "2023-11-16T22:54:59.232571Z"
    },
    "tags": []
   },
   "source": [
    "#### Step 5.1 Initialize HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45ee819b-78bb-4351-a794-739ef828d877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:55.677293Z",
     "iopub.status.busy": "2024-01-30T16:20:55.677042Z",
     "iopub.status.idle": "2024-01-30T16:20:55.680066Z",
     "shell.execute_reply": "2024-01-30T16:20:55.679531Z",
     "shell.execute_reply.started": "2024-01-30T16:20:55.677276Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from maxaillm.data.embeddings.MaxHuggingFaceEmbeddings import MaxHuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a650402c-7972-4041-b891-8166f766355f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:55.680992Z",
     "iopub.status.busy": "2024-01-30T16:20:55.680735Z",
     "iopub.status.idle": "2024-01-30T16:20:56.718290Z",
     "shell.execute_reply": "2024-01-30T16:20:56.717562Z",
     "shell.execute_reply.started": "2024-01-30T16:20:55.680974Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<maxaillm.data.embeddings.MaxHuggingFaceEmbeddings.MaxHuggingFaceEmbeddings object at 0x7fda71e43ca0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "embeddings = MaxHuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afc551-e2e1-4f25-bbb8-340036829640",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 5.2 Initialize VectorDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04e910de-9fd6-4683-91c3-500f95c05a51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:20:56.719703Z",
     "iopub.status.busy": "2024-01-30T16:20:56.719175Z",
     "iopub.status.idle": "2024-01-30T16:20:56.728398Z",
     "shell.execute_reply": "2024-01-30T16:20:56.727857Z",
     "shell.execute_reply.started": "2024-01-30T16:20:56.719629Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from maxaillm.data.vectorstore.MaxRedis import MaxRedis\n",
    "vectordb = MaxRedis(\n",
    "                    redis_url= config_store.config_data['REDIS_CONNECTION_STRING'],\n",
    "                    index_name = \"collection-name\",\n",
    "                    embedding_function = embeddings\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71e2bf5-8506-4d79-af0f-a1869991eaef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T04:35:22.642896Z",
     "iopub.status.busy": "2023-11-28T04:35:22.642374Z",
     "iopub.status.idle": "2023-11-28T04:35:22.645537Z",
     "shell.execute_reply": "2023-11-28T04:35:22.644933Z",
     "shell.execute_reply.started": "2023-11-28T04:35:22.642878Z"
    },
    "tags": []
   },
   "source": [
    "#### Step 5.3 Adding Documents to Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0260f47-e332-453b-b8cb-d93d749fdfd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectordb.add(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68e1530a-b88b-4c0c-bb10-b25476992825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:21:02.944400Z",
     "iopub.status.busy": "2024-01-30T16:21:02.944117Z",
     "iopub.status.idle": "2024-01-30T16:21:02.976127Z",
     "shell.execute_reply": "2024-01-30T16:21:02.975402Z",
     "shell.execute_reply.started": "2024-01-30T16:21:02.944380Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "li = vectordb.search(\"Group Sales Results\", k =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95020aad-7834-462e-b961-50bbb008da79",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 6 : Setting up Retriever\n",
    "In this phase, the focus is on configuring various retrievers and rerankers that play a crucial role in information retrieval processes. These components are essential for efficiently finding relevant information from large datasets or document collections.\n",
    "- **Retrievers** \n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever doesn't necessarily stores documents, it only returns document. \n",
    "\n",
    "We currently support different retrievers like -\n",
    "\n",
    ">- `MultiQuery` - Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded text chunks based on \"distance\".\n",
    "> - `HybridSearch` - A hybrid search retriever is a lightweight filter that selects documents relevant to a query from a database. \n",
    ">- `Base` - A Base retriever uses default search algorithm offered by vector db, and retrieves the relevant chunks which are most similar to the query\n",
    ">- `HyDE` - At a high level, HyDE is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example.\n",
    "- **Reranker** \n",
    "To improve search relevance by reordering the result set returned by a retriever with a different model. Reranking computes a relevance score between the query and each retrieved text chunk, and returns the list of text chunks sorted from the most to the least relevant.\n",
    "\n",
    "\n",
    "> - `Cohere`: This reranker reorders search results based on coherence with the query, enhancing relevance and accuracy.\n",
    "> - `LostInMiddle`: This reranker designed to address specific challenges in information retrieval, such as finding relevant information that's not immediately obvious or typically overlooked by standard search algorithm, like the information in the middle of the retrieved context is lost due to higher number of retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3cd8244-2ed8-42b6-8a70-a30c9b2c47cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:21:02.977310Z",
     "iopub.status.busy": "2024-01-30T16:21:02.977027Z",
     "iopub.status.idle": "2024-01-30T16:21:02.980814Z",
     "shell.execute_reply": "2024-01-30T16:21:02.980016Z",
     "shell.execute_reply.started": "2024-01-30T16:21:02.977290Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from maxaillm.data.retriever.Retriever import MaxRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6753c82c-7ed7-4951-9e28-1e7be02c464e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 6.1. Initialize Retriever (MultiQuery) and Ranking method (LostInMiddle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2d1f3e85-d470-4d71-a63c-5433a85ad474",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:21:02.981978Z",
     "iopub.status.busy": "2024-01-30T16:21:02.981712Z",
     "iopub.status.idle": "2024-01-30T16:21:02.986493Z",
     "shell.execute_reply": "2024-01-30T16:21:02.985757Z",
     "shell.execute_reply.started": "2024-01-30T16:21:02.981960Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieve = MaxRetriever(vectordb = vectordb, llm = llm, retriever_type=\"MultiQuery\", reranker_type=\"LostInMiddle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "55abfae1-ea39-4ae3-b837-8bd1933ecd82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:21:02.995902Z",
     "iopub.status.busy": "2024-01-30T16:21:02.987250Z",
     "iopub.status.idle": "2024-01-30T16:21:02.999616Z",
     "shell.execute_reply": "2024-01-30T16:21:02.999103Z",
     "shell.execute_reply.started": "2024-01-30T16:21:02.995880Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<maxaillm.data.retriever.Retriever.MaxRetriever at 0x7fda71e43fd0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30b4cb0b-2564-48a0-bad7-3d15fc84fab8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:21:03.000644Z",
     "iopub.status.busy": "2024-01-30T16:21:03.000396Z",
     "iopub.status.idle": "2024-01-30T16:21:04.447018Z",
     "shell.execute_reply": "2024-01-30T16:21:04.446377Z",
     "shell.execute_reply.started": "2024-01-30T16:21:03.000619Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata key file_name not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['file_name', 'file_metadata', 'links']\n",
      "Metadata key file_name not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['file_name', 'file_metadata', 'links']\n",
      "Metadata key file_name not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['file_name', 'file_metadata', 'links']\n",
      "Metadata key file_name not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['file_name', 'file_metadata', 'links']\n",
      "Metadata key file_name not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['file_name', 'file_metadata', 'links']\n",
      "Metadata key file_name not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['file_name', 'file_metadata', 'links']\n",
      "Metadata key file_name not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['file_name', 'file_metadata', 'links']\n",
      "Metadata key file_name not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['file_name', 'file_metadata', 'links']\n"
     ]
    }
   ],
   "source": [
    "out = retrieve.retrieve_and_rerank(\"Summarize 2022 performance\")[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3d278-248e-496e-9e28-7129eb7b38fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 7 : Setting up Generator\n",
    "The generator is a large transformer model, such as GPT3. 5, GPT4, Llama2, Falcon, PaLM, and BERT. The generator takes the input query and the retrieved documents, and generates a response. The retrieved documents and the input query are concatenated and fed into the generator. The MaxGenerator also provides streaming capabilities while generating responses.\n",
    "\n",
    "Different Generators can be tasked with \n",
    " - **Language Generation Models**: For tasks like summarization and question answering, language models like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers) could be used.\n",
    "- **Specialized NLP Tools**: For sentiment analysis and insight extraction, there are specialized tools and libraries that focus on specific aspects of text analysis, like sentiment scoring or topic modeling.\n",
    "- **Document Intelligence**: For document intelligence,we provide generators that combine OCR (Optical Character Recognition), NLP, and machine learning to extract and process information from documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7c1b8f7b-fc11-4fd4-ac06-5aef31d8b7ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:21:04.448298Z",
     "iopub.status.busy": "2024-01-30T16:21:04.447990Z",
     "iopub.status.idle": "2024-01-30T16:21:04.452081Z",
     "shell.execute_reply": "2024-01-30T16:21:04.451258Z",
     "shell.execute_reply.started": "2024-01-30T16:21:04.448277Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from maxaillm.app.generator.MaxGenerator import MaxGenerator, LangChainGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "acecb9be-b09a-4de7-99af-522cb1e78e4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-30T16:21:04.453248Z",
     "iopub.status.busy": "2024-01-30T16:21:04.452981Z",
     "iopub.status.idle": "2024-01-30T16:21:04.496244Z",
     "shell.execute_reply": "2024-01-30T16:21:04.456416Z",
     "shell.execute_reply.started": "2024-01-30T16:21:04.453229Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mg = LangChainGenerator(llm=llm, \n",
    "                        method='stuff', \n",
    "                        prompt_config={'moderations':'',\n",
    "                                       'task':'',\n",
    "                                       'identity':''},\n",
    "                        verbose=True, \n",
    "                        streamable = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b93218-06f3-4a64-80d7-594488e1744d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T04:56:14.291008Z",
     "iopub.status.busy": "2023-11-28T04:56:14.290428Z",
     "iopub.status.idle": "2023-11-28T04:56:14.293785Z",
     "shell.execute_reply": "2023-11-28T04:56:14.293256Z",
     "shell.execute_reply.started": "2023-11-28T04:56:14.290989Z"
    }
   },
   "source": [
    "#### Step 7.1 Streaming output for QA Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663f1a6-91ba-4652-b933-44a6d001e682",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "async for i in mg.generate_stream(query='Enter your question here', context=out):\n",
    "    print(i, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facac7bb-a35e-4611-88f3-c4d3447ba5da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = mg.generate(query='Enter your question here', context=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9775f3-fa5e-40a6-9601-034901da98ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
